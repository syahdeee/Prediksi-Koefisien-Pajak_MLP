{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGbVbSUGz35R",
        "outputId": "32816fb7-fcec-4d8f-efb0-31eaaca2442d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NivDk9-hd6Dv7I575OIlRrVoNZdkIMw5\n",
            "To: /content/Data_Diesel.xlsx\n",
            "100% 2.81M/2.81M [00:00<00:00, 146MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1L5D9PmJvQB_ExIiCzVClpSJ8vH1rdj2z\n",
            "To: /content/Data_Tes2.xlsx\n",
            "100% 72.5k/72.5k [00:00<00:00, 104MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1NivDk9-hd6Dv7I575OIlRrVoNZdkIMw5\n",
        "!gdown --id 1L5D9PmJvQB_ExIiCzVClpSJ8vH1rdj2z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNpoY3I-z8JQ",
        "outputId": "f3712f3f-4d28-44cf-d680-f61fa6625f94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5JQL-Juz-uz",
        "outputId": "99ee97c7-d5df-4528-b778-89a1a150c0bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.11.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPMdXJU-0AB8",
        "outputId": "5aa95bdc-d34d-4cd2-8c94-cb88a50950b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.1/176.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-tuner\n",
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stS4x2Tw0BkU",
        "outputId": "dd62840c-0f2b-48c3-96cc-ade816a0da22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import r2_score\n",
        "from keras.layers import Dense, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from math import floor\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.layers import LeakyReLU\n",
        "LeakyReLU = LeakyReLU(alpha=0.1)\n",
        "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MFsqmf00Cwu"
      },
      "outputs": [],
      "source": [
        "data_diesel = pd.read_excel('Data_Diesel.xlsx')\n",
        "# data_diesel = data_diesel[:500]\n",
        "# data_diesel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Hjr5LkY0KKo"
      },
      "outputs": [],
      "source": [
        "X = data_diesel[[\"Tahun Pembuatan\",\"Opasitas\", \"Usia\"]]\n",
        "y = data_diesel[\"Rating\"]\n",
        "\n",
        "X = X.values\n",
        "y = y.values.reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwGe46OY0bj3"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the StandardScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the data\n",
        "scaler.fit(X)\n",
        "# Transform the data\n",
        "x_scaled_diesel = scaler.transform(X)\n",
        "\n",
        "# Fit the scaler to the data\n",
        "scaler.fit(y)\n",
        "# Transform the data\n",
        "y_scaled_diesel = scaler.transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMujWLTi0c_2"
      },
      "outputs": [],
      "source": [
        "# X_optimize, X_train, y_optimize, y_train = train_test_split(x_scaled_gasoline, y_scaled_gasoline, test_size=0.4, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_scaled_diesel, y_scaled_diesel, test_size=0.02, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xihGD-s0j4i"
      },
      "outputs": [],
      "source": [
        "# Define the scoring function (example using R^2 score)\n",
        "score_acc = make_scorer(r2_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS88z7fF0lVh"
      },
      "outputs": [],
      "source": [
        "params_nn2 ={\n",
        "    'neurons': (1, 10),\n",
        "    'activation':(0, 7),\n",
        "    'optimizer':(0,7),\n",
        "    'learning_rate':(0.01, 0.1),\n",
        "    'batch_size':(20, 30),\n",
        "    'epochs':(20, 100),\n",
        "    'num_layers' : (2,8),\n",
        "    # 'normalization':(0,0.75),\n",
        "    'dropout':(0,1),\n",
        "    # 'dropout_rate':(0,0.1)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdTyyiR506PL"
      },
      "outputs": [],
      "source": [
        "# Create function\n",
        "def nn_cl_bo2(neurons, activation, optimizer, learning_rate, batch_size, epochs,num_layers, dropout):\n",
        "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl']\n",
        "    optimizerD = {'Adam': Adam(learning_rate=learning_rate), 'SGD': SGD(learning_rate=learning_rate),\n",
        "                  'RMSprop': RMSprop(learning_rate=learning_rate), 'Adadelta': Adadelta(learning_rate=learning_rate),\n",
        "                  'Adagrad': Adagrad(learning_rate=learning_rate), 'Adamax': Adamax(learning_rate=learning_rate),\n",
        "                  'Nadam': Nadam(learning_rate=learning_rate), 'Ftrl': Ftrl(learning_rate=learning_rate)}\n",
        "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu', 'elu', 'exponential']\n",
        "    neurons = round(neurons)\n",
        "    activation = activationL[round(activation)]\n",
        "    optimizer = optimizerD[optimizerL[round(optimizer)]]\n",
        "    batch_size = round(batch_size)\n",
        "    epochs = round(epochs)\n",
        "    num_layers = round(num_layers)\n",
        "\n",
        "    def nn_cl_fun():\n",
        "        nn = Sequential()\n",
        "        nn.add(Dense(neurons, input_dim=X_train.shape[1], activation=activation))\n",
        "\n",
        "        for i in range(num_layers):\n",
        "          if i == 0:\n",
        "              nn.add(Dense(neurons, input_dim=X_train.shape[1], activation=activation))\n",
        "              nn.add(Dropout(0.1, seed=123))\n",
        "          else:\n",
        "              nn.add(Dense(neurons, activation=activation))\n",
        "              nn.add(Dropout(0.1, seed=123))\n",
        "\n",
        "        nn.add(Dense(1, activation= 'sigmoid'))\n",
        "        nn.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "        return nn\n",
        "\n",
        "    es = EarlyStopping(monitor='loss', patience=20, verbose=0)\n",
        "    nn = KerasRegressor(model=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    nn.fit(X_train, y_train)\n",
        "    y_pred = nn.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return r2\n",
        "    # kfold = KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "    # scores = cross_val_score(nn, X_optimize, y_optimize, scoring=score_acc, cv=kfold, fit_params={'callbacks': [es]}) #data optimasi\n",
        "    # return scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldkpKYSQ08d3"
      },
      "outputs": [],
      "source": [
        "# Run Bayesian Optimization\n",
        "tuner = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSGyomSV09vF",
        "outputId": "43c8dfa1-5afd-4ee6-a0d2-dbc44214e1de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | activa... | batch_... |  dropout  |  epochs   | learni... |  neurons  | num_la... | optimizer |\n",
            "-------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.8845   \u001b[0m | \u001b[0m4.285    \u001b[0m | \u001b[0m21.69    \u001b[0m | \u001b[0m0.4361   \u001b[0m | \u001b[0m81.54    \u001b[0m | \u001b[0m0.03658  \u001b[0m | \u001b[0m2.342    \u001b[0m | \u001b[0m2.135    \u001b[0m | \u001b[0m2.942    \u001b[0m |\n",
            "| \u001b[95m2        \u001b[0m | \u001b[95m0.9485   \u001b[0m | \u001b[95m1.671    \u001b[0m | \u001b[95m23.38    \u001b[0m | \u001b[95m0.9907   \u001b[0m | \u001b[95m39.02    \u001b[0m | \u001b[95m0.01731  \u001b[0m | \u001b[95m7.026    \u001b[0m | \u001b[95m5.727    \u001b[0m | \u001b[95m1.92     \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m-3.24e-05\u001b[0m | \u001b[0m3.264    \u001b[0m | \u001b[0m21.18    \u001b[0m | \u001b[0m0.07396  \u001b[0m | \u001b[0m92.06    \u001b[0m | \u001b[0m0.08146  \u001b[0m | \u001b[0m8.565    \u001b[0m | \u001b[0m6.891    \u001b[0m | \u001b[0m6.937    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.847    \u001b[0m | \u001b[0m4.041    \u001b[0m | \u001b[0m28.14    \u001b[0m | \u001b[0m0.4213   \u001b[0m | \u001b[0m22.2     \u001b[0m | \u001b[0m0.05087  \u001b[0m | \u001b[0m1.948    \u001b[0m | \u001b[0m6.903    \u001b[0m | \u001b[0m4.884    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.9165   \u001b[0m | \u001b[0m3.957    \u001b[0m | \u001b[0m22.74    \u001b[0m | \u001b[0m0.9985   \u001b[0m | \u001b[0m31.04    \u001b[0m | \u001b[0m0.06539  \u001b[0m | \u001b[0m5.365    \u001b[0m | \u001b[0m4.43     \u001b[0m | \u001b[0m5.095    \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.8362   \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m31.82    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m8.457    \u001b[0m | \u001b[0m8.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
            "| \u001b[95m7        \u001b[0m | \u001b[95m0.9585   \u001b[0m | \u001b[95m0.7924   \u001b[0m | \u001b[95m23.58    \u001b[0m | \u001b[95m0.3497   \u001b[0m | \u001b[95m38.54    \u001b[0m | \u001b[95m0.02035  \u001b[0m | \u001b[95m8.503    \u001b[0m | \u001b[95m5.788    \u001b[0m | \u001b[95m1.521    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m0.8699   \u001b[0m | \u001b[0m4.83     \u001b[0m | \u001b[0m22.28    \u001b[0m | \u001b[0m0.5184   \u001b[0m | \u001b[0m65.95    \u001b[0m | \u001b[0m0.01     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m-0.000101\u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m52.68    \u001b[0m | \u001b[0m0.1      \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m7.0      \u001b[0m |\n",
            "=========================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "tuner.maximize(init_points=5, n_iter=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7TDk7IX2yMt",
        "outputId": "f7f8771a-8b9a-4f99-9c45-3469f764a109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'activation': 0.7924384747908901, 'batch_size': 23.582032013087698, 'dropout': 0.3496553954929623, 'epochs': 38.541716590926725, 'learning_rate': 0.020348264156592587, 'neurons': 8.50338869965181, 'num_layers': 5.787759086777139, 'optimizer': 1.5212597466575446}\n"
          ]
        }
      ],
      "source": [
        "# Get the best parameters\n",
        "best_params = tuner.max['params']\n",
        "print(\"Best Parameters:\", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "60y_MDfe25ng",
        "outputId": "151e6e49-21a5-47b4-b4f1-470934475b92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'activation': 'sigmoid',\n",
              " 'batch_size': 24,\n",
              " 'dropout': 0.3496553954929623,\n",
              " 'epochs': 39,\n",
              " 'learning_rate': 0.020348264156592587,\n",
              " 'neurons': 9,\n",
              " 'num_layers': 6,\n",
              " 'optimizer': <keras.optimizers.legacy.rmsprop.RMSprop at 0x7f6b6873d5a0>}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params_nn_ = tuner.max['params']\n",
        "learning_rate = params_nn_['learning_rate']\n",
        "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        "               'elu', 'exponential', LeakyReLU,'relu']\n",
        "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
        "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
        "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
        "params_nn_['num_layers'] = round(params_nn_['num_layers'])\n",
        "params_nn_['neurons'] = round(params_nn_['neurons'])\n",
        "optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\n",
        "optimizerD= {'Adam':Adam(learning_rate=learning_rate), 'SGD':SGD(learning_rate=learning_rate),\n",
        "             'RMSprop':RMSprop(learning_rate=learning_rate), 'Adadelta':Adadelta(learning_rate=learning_rate),\n",
        "             'Adagrad':Adagrad(learning_rate=learning_rate), 'Adamax':Adamax(learning_rate=learning_rate),\n",
        "             'Nadam':Nadam(learning_rate=learning_rate), 'Ftrl':Ftrl(learning_rate=learning_rate)}\n",
        "params_nn_['optimizer'] = optimizerD[optimizerL[round(params_nn_['optimizer'])]]\n",
        "params_nn_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BVP56DMS26sL",
        "outputId": "f5d52fe2-efac-4b36-bc70-e235bd31f3b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U keras-tuner\n",
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Um-V5Ci72769"
      },
      "outputs": [],
      "source": [
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GXuHhoIb29P3"
      },
      "outputs": [],
      "source": [
        "# Fitting Neural Network\n",
        "def nn_cl_fun():\n",
        "  nn = Sequential()\n",
        "  nn.add(Dense(params_nn_['neurons'], input_dim=X_train.shape[1], activation=params_nn_['activation']))\n",
        "  for i in range(params_nn_['num_layers'] ):\n",
        "      if i == 0:\n",
        "        nn.add(Dense(params_nn_['neurons'], input_dim=X_train.shape[1], activation=params_nn_['activation']))\n",
        "        if params_nn_['dropout'] > 0.5:\n",
        "          nn.add(Dropout(0.1, seed=123))\n",
        "        else:\n",
        "          nn.add(Dense(params_nn_['neurons'], activation=params_nn_['activation']))\n",
        "          nn.add(Dropout(0.1, seed=123))\n",
        "\n",
        "  nn.add(Dense(1, activation='sigmoid'))\n",
        "  nn.compile(loss='mean_squared_error', optimizer=params_nn_['optimizer'], metrics=tfa.metrics.r_square.RSquare())\n",
        "  return nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "13eurGpi2-Mu",
        "outputId": "f2db68b9-df14-42bb-a7d8-c8cb2100000b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_62 (Dense)            (None, 9)                 36        \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 9)                 90        \n",
            "                                                                 \n",
            " dropout_44 (Dropout)        (None, 9)                 0         \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 1)                 10        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 226\n",
            "Trainable params: 226\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "es = EarlyStopping(monitor='loss', verbose=0, patience=20)\n",
        "model =nn_cl_fun ()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4fzI3bUm2_dA"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs= params_nn_['epochs'], callbacks = [es], validation_split=0.2, batch_size=params_nn_['batch_size'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eFRXzFP_3Arr",
        "outputId": "85c4d1c7-ce26-42f8-a526-9bd4f055f22d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51/51 [==============================] - 0s 2ms/step - loss: 7.7633e-04 - r_square: 0.9866\n",
            "[test loss, test accuracy]: [0.0007763303583487868, 0.9865851998329163]\n"
          ]
        }
      ],
      "source": [
        "eval_result = model.evaluate(X_test, y_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N76xlq7RH-Yg"
      },
      "outputs": [],
      "source": [
        "model.save('model_diesel.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld0HUMsKbGJ4"
      },
      "source": [
        "FUNGSI UNTUK MEMVISUALISASI PERUBAHAN LOSS DAN R2 SCORE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aDuAJTi7H_7u"
      },
      "outputs": [],
      "source": [
        "def make_plot(train, validation, title):\n",
        "  graph = plt.plot(history.history[train])\n",
        "  graph = plt.plot(history.history[validation])\n",
        "  graph = plt.title(title)\n",
        "  graph = plt.legend(['training', 'validation'])\n",
        "  graph = plt.show()\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZeCN7cVqcF6_"
      },
      "outputs": [],
      "source": [
        "make_plot('loss', 'val_loss', 'Perubahan Loss pada tiap Epoch')\n",
        "make_plot('r_square', 'val_r_square', 'Perubahan R2 Score pada tiap Epoch')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}